{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:17:10.316556Z",
     "start_time": "2021-06-12T12:17:10.005591Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:17:11.023658Z",
     "start_time": "2021-06-12T12:17:10.983308Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:17:14.507710Z",
     "start_time": "2021-06-12T12:17:12.432578Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:17:17.134119Z",
     "start_time": "2021-06-12T12:17:17.116184Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:17:18.341034Z",
     "start_time": "2021-06-12T12:17:18.324124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:17:23.330829Z",
     "start_time": "2021-06-12T12:17:20.697944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for 1-layer W\n",
      "Gradient check passed!\n",
      "Checking gradient for 1-layer B\n",
      "Gradient check passed!\n",
      "Checking gradient for 3-layer W\n",
      "Gradient check passed!\n",
      "Checking gradient for 3-layer B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:17:28.109245Z",
     "start_time": "2021-06-12T12:17:25.385874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for 1-layer W\n",
      "Gradient check passed!\n",
      "Checking gradient for 1-layer B\n",
      "Gradient check passed!\n",
      "Checking gradient for 3-layer W\n",
      "Gradient check passed!\n",
      "Checking gradient for 3-layer B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:17:31.408691Z",
     "start_time": "2021-06-12T12:17:31.393885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03333333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:26:25.140440Z",
     "start_time": "2021-06-12T12:25:46.899398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.274774, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.243732, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.232188, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227860, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225036, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220233, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.208220, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.185030, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.155611, Train accuracy: 0.209778, val accuracy: 0.218000\n",
      "Loss: 2.124790, Train accuracy: 0.239667, val accuracy: 0.240000\n",
      "Loss: 2.092044, Train accuracy: 0.263667, val accuracy: 0.259000\n",
      "Loss: 2.052255, Train accuracy: 0.268667, val accuracy: 0.272000\n",
      "Loss: 2.002428, Train accuracy: 0.296778, val accuracy: 0.302000\n",
      "Loss: 1.942131, Train accuracy: 0.323778, val accuracy: 0.323000\n",
      "Loss: 1.876229, Train accuracy: 0.348778, val accuracy: 0.355000\n",
      "Loss: 1.807530, Train accuracy: 0.382778, val accuracy: 0.385000\n",
      "Loss: 1.740335, Train accuracy: 0.413111, val accuracy: 0.402000\n",
      "Loss: 1.678208, Train accuracy: 0.447444, val accuracy: 0.436000\n",
      "Loss: 1.620248, Train accuracy: 0.466444, val accuracy: 0.449000\n",
      "Loss: 1.568104, Train accuracy: 0.488444, val accuracy: 0.473000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:27:43.591200Z",
     "start_time": "2021-06-12T12:27:43.446825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fad06b346d0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:28:24.561915Z",
     "start_time": "2021-06-12T12:27:50.261737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.274888, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.244074, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.232552, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228073, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225575, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.221785, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212894, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.194183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.167735, Train accuracy: 0.207667, val accuracy: 0.218000\n",
      "Loss: 2.137783, Train accuracy: 0.234778, val accuracy: 0.233000\n",
      "Loss: 2.106258, Train accuracy: 0.259667, val accuracy: 0.263000\n",
      "Loss: 2.072079, Train accuracy: 0.272333, val accuracy: 0.273000\n",
      "Loss: 2.035329, Train accuracy: 0.277778, val accuracy: 0.279000\n",
      "Loss: 1.996445, Train accuracy: 0.279556, val accuracy: 0.286000\n",
      "Loss: 1.952192, Train accuracy: 0.306889, val accuracy: 0.311000\n",
      "Loss: 1.901588, Train accuracy: 0.337111, val accuracy: 0.337000\n",
      "Loss: 1.845050, Train accuracy: 0.368111, val accuracy: 0.366000\n",
      "Loss: 1.787422, Train accuracy: 0.393222, val accuracy: 0.383000\n",
      "Loss: 1.730360, Train accuracy: 0.416556, val accuracy: 0.400000\n",
      "Loss: 1.677737, Train accuracy: 0.437000, val accuracy: 0.418000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:28:48.940894Z",
     "start_time": "2021-06-12T12:28:48.809388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fad06abefd0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:34:21.781560Z",
     "start_time": "2021-06-12T12:33:39.815673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.298959, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292121, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285941, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280365, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275341, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270811, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266725, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263044, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259722, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256725, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.254021, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251577, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.249363, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.247362, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.245549, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.243902, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.242405, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.241047, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239804, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238676, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T12:34:54.938409Z",
     "start_time": "2021-06-12T12:34:53.425789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.306291, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.295084, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.283077, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.272421, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.259097, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.246023, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.229421, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.181217, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.087211, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.988657, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.889978, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.793647, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.739083, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.702580, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.594376, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.661862, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.562968, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.498555, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.490078, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.435668, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.406688, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.391169, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.360736, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.259216, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.183437, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.141541, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.088804, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.997943, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.958119, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.913828, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 0.860647, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 0.795817, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.768337, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.717629, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.679814, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.645804, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.612257, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.586891, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.571777, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.543767, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.519400, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.508581, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.487207, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.459248, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.434394, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.415551, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.387288, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.366608, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.328900, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.302050, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.273738, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.240482, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.218721, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.191890, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.164133, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.145028, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.125242, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.109723, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.096606, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.087713, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.080240, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.070847, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.065038, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.060188, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.055494, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.052005, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.047570, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.044551, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.042801, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.039795, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.037619, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.035065, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.033367, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.031892, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.030213, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.028828, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.027792, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.026721, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.025487, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.024277, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.023283, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.022697, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.021620, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.021025, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.020316, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.019713, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.018965, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.018275, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.017828, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.017179, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.016641, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.016305, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.015688, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.015284, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.014919, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.014480, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.014136, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.013731, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.013379, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.013061, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.012799, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.012485, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.012229, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.011947, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.011635, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.011395, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.011185, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.010901, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.010731, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.010469, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.010245, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.010048, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.009863, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.009705, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.009507, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.009317, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.009207, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.009011, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.008878, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.008672, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.008551, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.008375, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.008263, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.008122, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.008000, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.007860, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.007729, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.007640, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.007492, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.007368, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.007283, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.007183, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.007069, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006948, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006869, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.006779, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006683, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006572, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006497, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006408, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006324, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006233, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006159, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006086, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.006007, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.005929, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.005842, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.005775, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.005701, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.005635, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T13:12:34.910221Z",
     "start_time": "2021-06-12T13:12:34.553836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.305078, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.282237, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.252409, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.196786, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.052759, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.813794, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.230779, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.133260, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.710550, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.559990, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.561353, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.126710, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.008464, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.084740, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.689805, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.533486, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.432529, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.350663, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.256369, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.176474, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=3e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 3e-4\n",
    "reg_strength = 2e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
